#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup
import urllib.request
import requests
import re  
import os

#è¯¥ç±»ä¸»è¦ç”¨äºŽæ•°æ®ç»Ÿè®¡ä»¥åŠå¯åŠ¨æ¢å¤ç­?
class staticLoadNum():
    def __init__(self, url, level_num):
        self.url = url
        self.level_num = level_num
        self.list = list();
        self.file_num = 0;
        self.sum_num = 0;
        self.file_failed_num = 0;
        self.sum_failed_num = 0;

    def set_file_num(self, file_num):
        self.file_num = file_num
        self.sum_num += file_num

    def data_add(self, sub_data):
        #æ­¤å¤„å¯ä»¥å¢žåŠ ç±»åž‹åˆ¤æ–­
        self.list.append(sub_data)
        self.sum_num += sub_data.sum_num

    def __str__(self):
        static_info = 'The URL is:' + self.url + '\n' +\
                      'Level_num:' + str(self.level_num) + '\n' +\
                      'Num of Sons:' + str(len(self.list)) + '\n' +\
                      'File Num:' + str(self.file_num) + '\n' +\
                      'Sum File Num:' + str(self.sum_num)
        return static_info

#ä¸‹è½½ç±?
class downloadfile():
    ulr_set = set();
    def __init__(self, url, max_level, form_str, father_dir, level_num = 0):
        self.url = url
        self.max_level = max_level
        self.level_num =  level_num
        self.form_str = form_str
        self.form_file_list = list()
        self.sub_url_list = list()
        self.file_num = 0;
        self.localData = staticLoadNum(url, level_num)    #æ‰“åŒ…æ•°æ®
        downloadfile.ulr_set.add(url);

        #è§„åˆ’ä¸‹è½½æ–‡ä»¶å­˜å‚¨ç›®å½•å±‚çº§
        os.chdir(father_dir)
        load_dir = str(level_num) + '_load_' + form_str[1:]
        
        if not os.path.exists(load_dir):
            os.mkdir(load_dir)
        os.chdir(os.path.join(os.getcwd(), load_dir))
        self.dir_name = os.getcwd()

    def get_local_data(self):
        return self.localData

    #ç½‘ç»œé“¾æŽ¥åŸºç¡€æ–¹æ³•
    def connect_to_url(self):
        try:
            response = requests.get(self.url, timeout = 30)
            response.raise_for_status()
        except requests.RequestException as e:
            print('Conneted failed')
            return
        except:
            print('Socket failed!')
            return
        else:
            print(response)
            return response

    #è§£å†³æ–‡ä»¶æŸ¥æ‰¾æ–¹æ³•
    def get_a_href(self, response):
        if (response):
            soup = BeautifulSoup(response.content, "html.parser",from_encoding="GBK")
            a_list = self.parser_div_element(soup)
            #a_list = soup.find_all('a')
            for a_href in a_list:
                #a_href è¿‡æ»¤æ¡ä»¶
                if (self.filter_a_condition(a_href.string) == False):
                    #print('No Match Condition!,We Pass')
                    continue

                try:
                    href_url = a_href['href']
                except:
                    print('a-label have not a vaule of ATTR href')
                    continue

                if href_url in downloadfile.ulr_set:
                    continue

                #åŒ¹é…hrefæ˜¯å¦æ˜?
                fig_str = r'#$'
                pad_fig = re.compile(fig_str)
                if(pad_fig.match(href_url)):
                    print('Match # Success!')
                    try:
                        str1 = a_href['onclick']
                    except:
                        print('a label have not a vaule of ATTR onclick')
                        continue
              
                    m = re.findall("'([^']+)'", str1)
                    href_url = m[0]
                    #print(a_href)
                    #print(href_url)
                    
                #æ–‡ä»¶æ ¼å¼åŒ¹é…
                str_form = r'.+' + str(self.form_str) + '$'   
                pad = re.compile(str_form)

                #è¿”å›žä¸Šä¸€ç›®å½•åŒ¹é…
                sub_dir = r'.*/'                             
                pad_dir = re.compile(sub_dir)
                    
                if(pad.match(href_url)):                                                      #åŒ¹é…æ–‡ä»¶ï¼Œç¡®å®šæ‰¾åˆ°ä¸‹è½½çš„æ–‡ä»¶                      
                    #print(debug_href_url)
                    if href_url not in self.form_file_list:
                        self.form_file_list.append(href_url)
                elif (re.match(r'http', href_url)):                                           #åŒ¹é…å­é“¾æŽ?
                    #print('debug http')
                    if href_url not in self.sub_url_list:
                        self.sub_url_list.append(href_url)   
                elif (pad_dir.match(href_url)):                                               #ç¡®å®šæ˜¯å¦å±žäºŽè¿”å›žä¸Šä¸€ç›®å½•
                    print('debug sub_dir')
                    if(re.match(href_url.split('/')[-2],self.url.split('/')[-3])):            #åˆ¤æ–­è¿”å›žåˆ°åŽŸå§‹çš„ä½ç½®
                        print('continue')
                        print(href_url.split('/'))
                        print(self.url.split('/'))
                        continue
                    rHtml_str = self.remove_html()
                    temp_url = rHtml_str + href_url

                    #ç‰¹æ®ŠåŒ–å¤„ç†å…ˆ
##                  temp_url = 'http://www.bzmfxz.com' + href_url
##                  print(temp_url)
##                  if temp_url not in self.sub_url_list:
##                      self.sub_url_list.append(temp_url)
                    
    #åˆ é™¤htmlåŽç¼€
    def remove_html(self):
        html_form = r'.+\.htm'
        pad = re.compile(html_form)
        end_index = 0
        if(pad.match(self.url.split('/')[-1])):
            end_index = end_index = len(self.url.split('/')[-1])
        return self.url[:end_index]

    def parser_div_element(self, soup):
        a_list = list();
        if (soup == None):
            return a_list

        div_list = soup.find_all('div')
        for div_href in div_list:
            #print(div_href)
            if (self.filter_div_condition(div_href)):
                a_temp_list = div_href.find_all('a');
                #print(a_temp_list)
                for a_href in a_temp_list:
                    if (a_href not in a_list):
                        a_list.append(a_href)
        return a_list

    def filter_div_condition(self, div_str):
        if (div_str == None):
            return False

        if (self.level_num == 0):
            try:
                div_class = div_str['class']
            except:
                print('div-label have not value of ATTR class')
                return False

            if (re.match(r'.*pub_element', str(div_class))):
                return True
            else:
                print('Match class Failed!')
                return False

        return True
            

##    #Case One
##    def filter_a_condition(self, a_href_str):
##        if (a_href_str == None):
##            return False
##
##        if (self.level_num == 0):
##            gjb_str = r'.*\r\nGJB'
##            pad = re.compile(gjb_str)
##            if (pad.match(a_href_str)):
##                print('Match GJB Success!')
##                return True
##            else:
##                return False
##        elif (self.level_num == 1):
##            load_str = r'.*è¿›å…¥ä¸‹è½½é¡µé¢'
##            pad_load = re.compile(load_str)
##            if (pad_load.match(a_href_str)):
##                print ('Match load page Success!')
##                return True
##            else:
##                return False
##        elif (self.level_num == 2):
##            load_str = '.*ç‚¹å‡»ä¸‹è½½'
##            if (re.match(load_str, a_href_str)):
##                print ('Match LOAD Success!')
##                return True
##            else:
##                return False

    #Case Two
    def filter_a_condition(self, a_href_str):
        if (a_href_str == None):
            return False

        if (self.level_num == 1):
            load_str = r'Download Publication'
            pad_load = re.compile(load_str)
            if (pad_load.match(a_href_str)):
                #print ('Match load page Success!')
                return True
            else:
                return False

        return True
    

    #æ–‡ä»¶ä¸‹è½½
    def get_dir_name(self):
##        html_form = r'.+\.html$'
##        pad = re.compile(html_form)
##        if(pad.match(self.url.split('/')[-1])):
##            dir_list = self.url.split('/')[3:-1]
##        else:
##            dir_list = self.url.split('/')[3:]
        dirl = self.remove_html()
        dir_list = self.url.split('/')[3:]
        dir_name = str(self.level_num)
        for dir_str in dir_list:
            if (len(dir_name) + len(dir_str) > 64):
                return dir_name;
            
            dir_str = re.sub('[\/:*?"<>|]','-',dir_str)
            dir_name = dir_name + '_' + dir_str

        return dir_name
    
    def getFile(self, url):
        file_name = url.split('/')[-1]
        #print(file_name)
        try:  
            u = urllib.request.urlopen(url, timeout = 180)  
        except urllib.error.HTTPError:  
            print(url, "url file not found")  
            return
        except:
            print('File Socket failed!')
            return

        #å¦‚æžœæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™ä¸è¿›è¡Œå†æ¬¡å†™å…?
        if os.path.exists(file_name):
            print(file_name + ' existed, we passed')
            return

        block_sz = 8192        
        with open(file_name, 'wb') as f:  
            while True:
                buffer = u.read(block_sz)  
                if buffer:  
                    f.write(buffer)
                else:  
                    break
        print ("Sucessful to download" + " " + file_name)
        
    def loadFile(self):
        #self.form_file_list.sort()
        form_num = len(self.form_file_list)
        self.localData.set_file_num(form_num)
        print('The total Form num is', form_num)
        if (form_num != 0):
            dir_name = self.get_dir_name()
            if not os.path.exists(dir_name):
                os.mkdir(dir_name)

            cur_dir = os.getcwd()
            os.chdir(os.path.join(cur_dir, dir_name))  
            i = 0
            for url in self.form_file_list:
                if (re.match(r'http', url)):
                    url = url
                elif(self.url[-5:-1] == '.htm'):
                    url = self.url[:-len(self.url.split('/')[:-1])] + url 
                else:  
                    url = self.url + url

                print(url)
                self.getFile(url)
                i = i + 1
                rate = format(i/form_num, '.0%')
                print('have load num is:', i, 'and rate:', rate)
            
            os.chdir(cur_dir)

    #è§£å†³æ€§èƒ½é—®é¢˜---è¿›ç¨‹è°ƒåº¦
    def process_load_file(self):
        response = self.connect_to_url()
        self.get_a_href(response)
        self.loadFile()
        if self.level_num > self.max_level:
            return
        for sub_url in self.sub_url_list:
            sub_load_class = downloadfile(sub_url, self.max_level, self.form_str, self.dir_name, self.level_num+1)
            sub_load_class.process_load_file()
            self.localData.data_add(sub_load_class.get_local_data())
            print(sub_load_class);

    def __str__(self):
        info_str = 'LoadFile Infomation:\n' +\
                               'url:'+ self.url + '\n' +\
                               'level_num:' + str(self.level_num) + '\n' +\
                               'have load file:' + str(len(self.form_file_list)) + '\n' +\
                               'local data:' + str(self.localData)
        return info_str

if __name__=='__main__':
    d_ulr = input('Please input the pdf webSite:')
    level_max = int(input('Please input the max level:'))
    form_str = input('Please input the file form:')
    it_downloadfile = downloadfile(d_ulr, level_max, form_str, os.getcwd())
    print(os.getcwd())
    it_downloadfile.process_load_file()
    print(it_downloadfile)
    
    
